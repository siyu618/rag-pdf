import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

from dotenv import load_dotenv
load_dotenv()
# è®¾ç½® DeepSeek API

# 1. åŠ è½½ PDF æ–‡æ¡£
pdf_path = "data/Stream-Processing-with-Apache-Flink.pdf"
loader = PyPDFLoader(pdf_path)
documents = loader.load()

# 2. åˆ†å‰²æ–‡æœ¬
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# 3. åµŒå…¥å‘é‡ç”Ÿæˆ & å‘é‡æ•°æ®åº“
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
vector_store = FAISS.from_documents(texts, embeddings)
vector_store.save_local("faiss_index")

# 4. æ£€ç´¢å™¨
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# 5. ä½¿ç”¨ DeepSeek API ä½œä¸ºè¯­è¨€æ¨¡å‹
llm = ChatOpenAI(
    model_name="deepseek-chat",
    temperature=0.7,
    max_tokens=512
)

# 6. Prompt æ¨¡æ¿
template = """
Use the following context to answer the question. If unsure, say "I don't know."
Context:
{context}
Question: {question}
Answer:
"""
prompt = PromptTemplate(template=template, input_variables=["context", "question"])

# 7. æ„å»º RetrievalQA Chainï¼ˆRAGï¼‰
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# 8. æ‰§è¡Œé—®ç­”
# æ„å»ºå¯¹è¯è®°å¿†
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# æ„å»ºæŒç»­å¯¹è¯ RAG é“¾
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True
)

# å¼€å§‹å¯¹è¯
print("ğŸ’¬ Chat with your PDF (type 'exit' to stop):\n")

while True:
    query = input("ğŸ‘¤ You: ")
    if query.lower() in ["exit", "quit"]:
        break

    result = qa_chain({"question": query})
    answer = result["answer"]
    print("ğŸ¤– Bot:", answer)
